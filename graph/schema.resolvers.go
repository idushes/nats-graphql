package graph

// This file will be automatically regenerated based on the schema, any resolver
// implementations
// will be copied through when generating and any unknown code will be moved to the end.
// Code generated by github.com/99designs/gqlgen version v0.17.86

import (
	"context"
	"errors"
	"fmt"
	"nats-graphql/graph/model"
	"time"

	"github.com/nats-io/nats.go/jetstream"
)

// KvCreate is the resolver for the kvCreate field.
func (r *mutationResolver) KvCreate(ctx context.Context, bucket string, history *int, ttl *int, storage *string) (*model.KeyValue, error) {
	cfg := jetstream.KeyValueConfig{
		Bucket: bucket,
	}

	if history != nil {
		cfg.History = uint8(*history)
	}
	if ttl != nil {
		cfg.TTL = time.Duration(*ttl) * time.Second
	}
	if storage != nil {
		switch *storage {
		case "memory":
			cfg.Storage = jetstream.MemoryStorage
		default:
			cfg.Storage = jetstream.FileStorage
		}
	}

	kv, err := r.JS.CreateKeyValue(ctx, cfg)
	if err != nil {
		return nil, err
	}

	status, err := kv.Status(ctx)
	if err != nil {
		return nil, err
	}

	ttlSec := int(status.TTL().Seconds())
	return &model.KeyValue{
		Bucket:       status.Bucket(),
		History:      int(status.History()),
		TTL:          ttlSec,
		Storage:      status.BackingStore(),
		Bytes:        int(status.Bytes()),
		Values:       int(status.Values()),
		IsCompressed: status.IsCompressed(),
	}, nil
}

// KvPut is the resolver for the kvPut field.
func (r *mutationResolver) KvPut(ctx context.Context, bucket string, key string, value string) (*model.KVEntry, error) {
	kv, err := r.JS.KeyValue(ctx, bucket)
	if err != nil {
		return nil, err
	}

	rev, err := kv.Put(ctx, key, []byte(value))
	if err != nil {
		return nil, err
	}

	// Fetch the entry back to get the full metadata
	entry, err := kv.GetRevision(ctx, key, rev)
	if err != nil {
		return nil, err
	}

	return &model.KVEntry{
		Key:      entry.Key(),
		Value:    string(entry.Value()),
		Revision: int(entry.Revision()),
		Created:  entry.Created().Format(time.RFC3339),
	}, nil
}

// KvDelete is the resolver for the kvDelete field.
func (r *mutationResolver) KvDelete(ctx context.Context, bucket string, key string) (bool, error) {
	kv, err := r.JS.KeyValue(ctx, bucket)
	if err != nil {
		return false, err
	}

	if err := kv.Delete(ctx, key); err != nil {
		return false, err
	}

	return true, nil
}

// StreamCreate is the resolver for the streamCreate field.
func (r *mutationResolver) StreamCreate(ctx context.Context, name string, subjects []string, retention *string, storage *string, maxMsgs *int, maxBytes *int, replicas *int) (*model.StreamInfo, error) {
	cfg := jetstream.StreamConfig{
		Name:     name,
		Subjects: subjects,
	}

	if retention != nil {
		switch *retention {
		case "interest":
			cfg.Retention = jetstream.InterestPolicy
		case "workqueue":
			cfg.Retention = jetstream.WorkQueuePolicy
		default:
			cfg.Retention = jetstream.LimitsPolicy
		}
	}

	if storage != nil && *storage == "memory" {
		cfg.Storage = jetstream.MemoryStorage
	}

	if maxMsgs != nil {
		cfg.MaxMsgs = int64(*maxMsgs)
	}
	if maxBytes != nil {
		cfg.MaxBytes = int64(*maxBytes)
	}
	if replicas != nil {
		cfg.Replicas = *replicas
	}

	si, err := r.JS.CreateStream(ctx, cfg)
	if err != nil {
		return nil, err
	}

	info := si.CachedInfo()

	return &model.StreamInfo{
		Name:         info.Config.Name,
		Subjects:     info.Config.Subjects,
		Retention:    info.Config.Retention.String(),
		MaxConsumers: info.Config.MaxConsumers,
		MaxMsgs:      int(info.Config.MaxMsgs),
		MaxBytes:     int(info.Config.MaxBytes),
		Storage:      info.Config.Storage.String(),
		Replicas:     info.Config.Replicas,
		Messages:     int(info.State.Msgs),
		Bytes:        int(info.State.Bytes),
		Consumers:    info.State.Consumers,
		Created:      info.Created.Format(time.RFC3339),
	}, nil
}

// Publish is the resolver for the publish field.
func (r *mutationResolver) Publish(ctx context.Context, subject string, data string) (*model.PublishResult, error) {
	const maxPayload = 1 << 20 // 1 MB
	if len(data) > maxPayload {
		return nil, fmt.Errorf("payload too large: %d bytes (max %d)", len(data), maxPayload)
	}

	ack, err := r.JS.Publish(ctx, subject, []byte(data))
	if err != nil {
		return nil, err
	}

	return &model.PublishResult{
		Stream:   ack.Stream,
		Sequence: int(ack.Sequence),
	}, nil
}

// KeyValues is the resolver for the keyValues field.
func (r *queryResolver) KeyValues(ctx context.Context) ([]*model.KeyValue, error) {
	var result []*model.KeyValue

	names := r.JS.KeyValueStoreNames(ctx)
	for name := range names.Name() {
		kv, err := r.JS.KeyValue(ctx, name)
		if err != nil {
			return nil, err
		}

		status, err := kv.Status(ctx)
		if err != nil {
			return nil, err
		}

		ttlSeconds := 0
		if status.TTL() > 0 {
			ttlSeconds = int(status.TTL() / time.Second)
		}

		result = append(result, &model.KeyValue{
			Bucket:       status.Bucket(),
			History:      int(status.History()),
			TTL:          ttlSeconds,
			Storage:      status.BackingStore(),
			Bytes:        int(status.Bytes()),
			Values:       int(status.Values()),
			IsCompressed: status.IsCompressed(),
		})
	}

	if err := names.Error(); err != nil {
		return nil, err
	}

	return result, nil
}

// Streams is the resolver for the streams field.
func (r *queryResolver) Streams(ctx context.Context) ([]*model.StreamInfo, error) {
	var result []*model.StreamInfo

	streams := r.JS.ListStreams(ctx)
	for si := range streams.Info() {
		subjects := make([]string, 0, len(si.Config.Subjects))
		subjects = append(subjects, si.Config.Subjects...)

		result = append(result, &model.StreamInfo{
			Name:         si.Config.Name,
			Subjects:     subjects,
			Retention:    si.Config.Retention.String(),
			MaxConsumers: si.Config.MaxConsumers,
			MaxMsgs:      int(si.Config.MaxMsgs),
			MaxBytes:     int(si.Config.MaxBytes),
			Storage:      si.Config.Storage.String(),
			Replicas:     si.Config.Replicas,
			Messages:     int(si.State.Msgs),
			Bytes:        int(si.State.Bytes),
			Consumers:    si.State.Consumers,
			Created:      si.Created.Format(time.RFC3339),
		})
	}

	if err := streams.Err(); err != nil {
		return nil, err
	}

	return result, nil
}

// KvKeys is the resolver for the kvKeys field.
func (r *queryResolver) KvKeys(ctx context.Context, bucket string) ([]string, error) {
	kv, err := r.JS.KeyValue(ctx, bucket)
	if err != nil {
		return nil, err
	}

	lister, err := kv.ListKeys(ctx)
	if err != nil {
		return nil, err
	}

	var keys []string
	for key := range lister.Keys() {
		keys = append(keys, key)
	}

	return keys, nil
}

// KvGet is the resolver for the kvGet field.
func (r *queryResolver) KvGet(ctx context.Context, bucket string, key string) (*model.KVEntry, error) {
	kv, err := r.JS.KeyValue(ctx, bucket)
	if err != nil {
		return nil, err
	}

	entry, err := kv.Get(ctx, key)
	if err != nil {
		if errors.Is(err, jetstream.ErrKeyNotFound) {
			return nil, nil
		}
		return nil, err
	}

	return &model.KVEntry{
		Key:      entry.Key(),
		Value:    string(entry.Value()),
		Revision: int(entry.Revision()),
		Created:  entry.Created().Format(time.RFC3339),
	}, nil
}

// StreamMessages is the resolver for the streamMessages field.
func (r *queryResolver) StreamMessages(ctx context.Context, stream string, last int, startSeq *int, startTime *string, endTime *string, subject *string) ([]*model.StreamMessage, error) {
	const maxMessages = 100
	if last <= 0 {
		return nil, fmt.Errorf("last must be > 0")
	}
	if last > maxMessages {
		return nil, fmt.Errorf("last exceeds maximum of %d messages", maxMessages)
	}

	// Parse time filters
	var endTimeVal time.Time
	hasEndTime := false
	if endTime != nil {
		t, err := parseRFC3339(*endTime)
		if err != nil {
			return nil, fmt.Errorf("invalid endTime format (expected RFC3339): %w", err)
		}
		endTimeVal = t
		hasEndTime = true
	}

	s, err := r.JS.Stream(ctx, stream)
	if err != nil {
		return nil, err
	}

	info, err := s.Info(ctx)
	if err != nil {
		return nil, err
	}

	if info.State.Msgs == 0 {
		return []*model.StreamMessage{}, nil
	}

	// Build ordered consumer config
	cfg := jetstream.OrderedConsumerConfig{}

	// Subject filter
	if subject != nil && *subject != "" {
		cfg.FilterSubjects = []string{*subject}
	}

	// Determine delivery policy
	if startTime != nil {
		t, err := parseRFC3339(*startTime)
		if err != nil {
			return nil, fmt.Errorf("invalid startTime format (expected RFC3339): %w", err)
		}
		cfg.DeliverPolicy = jetstream.DeliverByStartTimePolicy
		cfg.OptStartTime = &t
	} else if startSeq != nil {
		if *startSeq < 1 {
			return nil, fmt.Errorf("startSeq must be >= 1")
		}
		cfg.DeliverPolicy = jetstream.DeliverByStartSequencePolicy
		cfg.OptStartSeq = uint64(*startSeq)
	} else {
		// Default: read last N messages from the end
		// Calculate start sequence to get roughly 'last' messages
		lastSeq := info.State.LastSeq
		firstSeq := info.State.FirstSeq
		calcStart := lastSeq - uint64(last) + 1
		if calcStart < firstSeq || calcStart > lastSeq {
			calcStart = firstSeq
		}
		cfg.DeliverPolicy = jetstream.DeliverByStartSequencePolicy
		cfg.OptStartSeq = calcStart
	}

	// Create ordered consumer (ephemeral, auto-cleanup)
	cons, err := s.OrderedConsumer(ctx, cfg)
	if err != nil {
		return nil, fmt.Errorf("failed to create consumer: %w", err)
	}

	// Fetch messages
	fetchCtx, cancel := context.WithTimeout(ctx, 5*time.Second)
	defer cancel()

	msgs, err := cons.Fetch(last, jetstream.FetchMaxWait(3*time.Second))
	if err != nil {
		return nil, fmt.Errorf("failed to fetch messages: %w", err)
	}

	var result []*model.StreamMessage
loop:
	for msg := range msgs.Messages() {
		select {
		case <-fetchCtx.Done():
			break loop
		default:
		}

		meta, err := msg.Metadata()
		if err != nil {
			continue
		}

		msgTime := meta.Timestamp

		// Apply endTime filter
		if hasEndTime && msgTime.After(endTimeVal) {
			break
		}

		result = append(result, &model.StreamMessage{
			Sequence:  int(meta.Sequence.Stream),
			Subject:   msg.Subject(),
			Data:      string(msg.Data()),
			Published: msgTime.Format(time.RFC3339Nano),
		})
	}

	if msgs.Error() != nil {
		// Ignore timeout errors — they just mean no more messages
		if msgs.Error().Error() != "nats: timeout" {
			return nil, msgs.Error()
		}
	}

	return result, nil
}

// StreamSubscribe is the resolver for the streamSubscribe field.
func (r *subscriptionResolver) StreamSubscribe(ctx context.Context, stream string, subject *string) (<-chan *model.StreamMessage, error) {
	s, err := r.JS.Stream(ctx, stream)
	if err != nil {
		return nil, err
	}

	// Build ordered consumer config — deliver only new messages
	cfg := jetstream.OrderedConsumerConfig{
		DeliverPolicy: jetstream.DeliverNewPolicy,
	}
	if subject != nil && *subject != "" {
		cfg.FilterSubjects = []string{*subject}
	}

	cons, err := s.OrderedConsumer(ctx, cfg)
	if err != nil {
		return nil, fmt.Errorf("failed to create consumer: %w", err)
	}

	// Start consuming
	iter, err := cons.Messages()
	if err != nil {
		return nil, fmt.Errorf("failed to start consuming: %w", err)
	}

	ch := make(chan *model.StreamMessage, 1)

	go func() {
		defer close(ch)
		defer iter.Stop()

		for {
			select {
			case <-ctx.Done():
				return
			default:
			}

			msg, err := iter.Next()
			if err != nil {
				return
			}

			meta, err := msg.Metadata()
			if err != nil {
				continue
			}

			sm := &model.StreamMessage{
				Sequence:  int(meta.Sequence.Stream),
				Subject:   msg.Subject(),
				Data:      string(msg.Data()),
				Published: meta.Timestamp.Format(time.RFC3339Nano),
			}

			select {
			case ch <- sm:
			case <-ctx.Done():
				return
			}
		}
	}()

	return ch, nil
}

// Mutation returns MutationResolver implementation.
func (r *Resolver) Mutation() MutationResolver { return &mutationResolver{r} }

// Query returns QueryResolver implementation.
func (r *Resolver) Query() QueryResolver { return &queryResolver{r} }

// Subscription returns SubscriptionResolver implementation.
func (r *Resolver) Subscription() SubscriptionResolver { return &subscriptionResolver{r} }

type mutationResolver struct{ *Resolver }
type queryResolver struct{ *Resolver }
type subscriptionResolver struct{ *Resolver }
